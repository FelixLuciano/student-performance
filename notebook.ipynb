{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student Performance ü§ìüìä\n",
    "\n",
    "Aluno: <span style=\"color:red\">Luciano Felix Dias</span>\n",
    "\n",
    "Entrega: 16 de maio de 2024\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inicio do projeto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leitura dos dados e corre√ß√£o de tipos de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path.cwd() / \"data\"\n",
    "DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS_TYPES = {\n",
    "    \"category\": [\n",
    "        \"school\",\n",
    "        \"sex\",\n",
    "        \"address\",\n",
    "        \"famsize\",\n",
    "        \"Pstatus\",\n",
    "        \"Mjob\",\n",
    "        \"Fjob\",\n",
    "        \"reason\",\n",
    "        \"guardian\",\n",
    "        \"schoolsup\",\n",
    "        \"famsup\",\n",
    "        \"paid\",\n",
    "        \"activities\",\n",
    "        \"nursery\",\n",
    "        \"higher\",\n",
    "        \"internet\",\n",
    "        \"romantic\",\n",
    "    ],\n",
    "    \"float64\": [\n",
    "        \"Medu\",\n",
    "        \"Fedu\",\n",
    "        \"traveltime\",\n",
    "        \"studytime\",\n",
    "        \"failures\",\n",
    "        \"famrel\",\n",
    "        \"freetime\",\n",
    "        \"goout\",\n",
    "        \"Dalc\",\n",
    "        \"Walc\",\n",
    "        \"health\",\n",
    "    ],\n",
    "    \"int64\": [\n",
    "        \"age\",\n",
    "        \"absences\",\n",
    "        \"grade\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "\n",
    "def load_data(filepath: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    format_data(df, COLUMNS_TYPES)\n",
    "\n",
    "    return split_features_target(df, \"grade\")\n",
    "\n",
    "\n",
    "def format_data(df: pd.DataFrame, format):\n",
    "    for column_type, column_group in format.items():\n",
    "        for column in column_group:\n",
    "            df[column] = df[column].astype(column_type)\n",
    "\n",
    "\n",
    "def split_features_target(df: pd.DataFrame, targets_column: str):\n",
    "    target = df[targets_column].copy()\n",
    "\n",
    "    df.drop(columns=[targets_column], inplace=True)\n",
    "\n",
    "    return df, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_data(DATA_DIR / \"student_performance.csv\")\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualiza√ß√£o simples dos dados\n",
    "\n",
    "Chamar isso de an√°lise explorat√≥ria √© vexat√≥rio..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_barplots(X: pd.DataFrame, y: pd.Series) -> None:\n",
    "    fig, axes = plt.subplots(10, 3, figsize=(12, 45))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    fig.tight_layout(h_pad=8, w_pad=5)\n",
    "\n",
    "    for index, (column_name, column_series) in enumerate(X.items()):\n",
    "        column_series \\\n",
    "            .value_counts() \\\n",
    "            .sort_index(ascending=False) \\\n",
    "            .plot \\\n",
    "            .barh(ax=axes[index])\n",
    "        axes[index].set_title(column_name)\n",
    "\n",
    "    fig.suptitle(\"Barplots of Categorical and Ordinal Features\", fontsize=16)\n",
    "    fig.subplots_adjust(top=0.96)\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(3, 4))\n",
    "    y.plot.hist(bins=20, edgecolor=\"black\")\n",
    "    plt.title(\"Histogram of Target\")\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_boxplots(X: pd.DataFrame, y: pd.Series) -> None:\n",
    "    df = pd.concat([X, y], axis=1)\n",
    "\n",
    "    fig, axes = plt.subplots(10, 3, figsize=(12, 45))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    fig.tight_layout(h_pad=8, w_pad=5)\n",
    "\n",
    "    for index, column in enumerate(X.columns):\n",
    "        ax = axes[index]\n",
    "        bp_objs = df.boxplot(\n",
    "            by=[\"school\", column],\n",
    "            column=\"grade\",\n",
    "            ax=ax,\n",
    "            rot=45,\n",
    "            fontsize=8,\n",
    "            return_type=\"dict\",\n",
    "            patch_artist=True,\n",
    "        )\n",
    "        boxes = bp_objs[\"grade\"][\"boxes\"]\n",
    "        box_colors = [\n",
    "            \"lightgreen\" if \"(GP,\" in tick.get_text() else \"lightblue\"\n",
    "            for tick in ax.get_xticklabels()\n",
    "        ]\n",
    "        for box, color in zip(boxes, box_colors):\n",
    "            box.set_facecolor(color)\n",
    "        ax.set_title(column)\n",
    "        ax.set_xlabel(\"\")\n",
    "    fig.suptitle(\"Grade distribution by feature and school\", fontsize=16)\n",
    "    fig.subplots_adjust(top=0.96)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_barplots(X, y)\n",
    "make_boxplots(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separa√ß√£o treino-teste e modelagem inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PowerTransformer\n",
    "from sklearn.dummy import DummyRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = X.select_dtypes(include=[\"category\"]).columns\n",
    "numerical_features = X.select_dtypes(include=[\"float64\", \"int64\"]).columns\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"power_transformer\", PowerTransformer()),\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    (\"encoder\", OneHotEncoder(drop=\"first\")),\n",
    "])\n",
    "\n",
    "preprocessing_pipeline = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", num_pipeline, numerical_features),\n",
    "        (\"cat\", cat_pipeline, categorical_features),\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    ")\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"preprocessor\", preprocessing_pipeline),\n",
    "    (\"regressor\", DummyRegressor(strategy=\"mean\")),\n",
    "])\n",
    "\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, ShuffleSplit\n",
    "from sklearn.linear_model import Ridge, HuberRegressor\n",
    "\n",
    "# Quanto maior o numero de splits, maior a signific√¢ncia estat√≠stica da\n",
    "# valida√ß√£o cruzada, mas tamb√©m maior o tempo de execu√ß√£o.\n",
    "num_splits = 1_000\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        \"regressor\": [DummyRegressor(strategy=\"mean\")],\n",
    "    }, {\n",
    "        \"regressor\": [Ridge()],\n",
    "        \"regressor__alpha\": np.logspace(-3, 3, 7),\n",
    "    }, {\n",
    "        \"regressor\": [HuberRegressor()],\n",
    "        \"regressor__alpha\": [0.0001, 0.001, 0.01],\n",
    "        \"regressor__epsilon\": [1.0, 1.5, 2.0],\n",
    "    },\n",
    "]\n",
    "\n",
    "test_fraction = 0.2\n",
    "num_samples_total = len(y_train)\n",
    "num_samples_test = int(test_fraction * num_samples_total)\n",
    "num_samples_train = num_samples_total - num_samples_test\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    pipe,\n",
    "    param_grid,\n",
    "    cv=ShuffleSplit(\n",
    "        n_splits=num_splits,\n",
    "        test_size=num_samples_test,\n",
    "        random_state=42,\n",
    "    ),\n",
    "    n_jobs=-1,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(grid.cv_results_) \\\n",
    "    .sort_values(by='rank_test_score')\n",
    "\n",
    "results_df = results_df \\\n",
    "    .set_index(\n",
    "        results_df[\"params\"] \\\n",
    "            .apply(lambda x: \"_\".join(str(val) for val in x.values()))\n",
    "    ) \\\n",
    "    .rename_axis(\"model\")\n",
    "\n",
    "model_scores = results_df.filter(regex=r\"split\\d*_test_score\")\n",
    "\n",
    "model_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_perf = model_scores.agg([\"mean\", \"std\"], axis=1)\n",
    "mean_perf[\"std\"] = mean_perf[\"std\"] / np.sqrt(num_splits)\n",
    "mean_perf = mean_perf.sort_values(\"mean\", ascending=False)\n",
    "mean_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compara√ß√£o de modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O c√≥digo a seguir foi copiado de https://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_stats.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scipy.stats import t\n",
    "\n",
    "\n",
    "def corrected_std(differences, n_train, n_test):\n",
    "    \"\"\"Corrects standard deviation using Nadeau and Bengio's approach.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    differences : ndarray of shape (n_samples,)\n",
    "        Vector containing the differences in the score metrics of two models.\n",
    "    n_train : int\n",
    "        Number of samples in the training set.\n",
    "    n_test : int\n",
    "        Number of samples in the testing set.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    corrected_std : float\n",
    "        Variance-corrected standard deviation of the set of differences.\n",
    "    \"\"\"\n",
    "    # kr = k times r, r times repeated k-fold crossvalidation,\n",
    "    # kr equals the number of times the model was evaluated\n",
    "    kr = len(differences)\n",
    "    corrected_var = np.var(differences, ddof=1) * (1 / kr + n_test / n_train)\n",
    "    corrected_std = np.sqrt(corrected_var)\n",
    "    return corrected_std\n",
    "\n",
    "\n",
    "def compute_corrected_ttest(differences, df, n_train, n_test):\n",
    "    \"\"\"Computes right-tailed paired t-test with corrected variance.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    differences : array-like of shape (n_samples,)\n",
    "        Vector containing the differences in the score metrics of two models.\n",
    "    df : int\n",
    "        Degrees of freedom.\n",
    "    n_train : int\n",
    "        Number of samples in the training set.\n",
    "    n_test : int\n",
    "        Number of samples in the testing set.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    t_stat : float\n",
    "        Variance-corrected t-statistic.\n",
    "    p_val : float\n",
    "        Variance-corrected p-value.\n",
    "    \"\"\"\n",
    "    mean = np.mean(differences)\n",
    "    std = corrected_std(differences, n_train, n_test)\n",
    "    t_stat = mean / std\n",
    "    p_val = t.sf(np.abs(t_stat), df)  # right-tailed t-test\n",
    "    return t_stat, p_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1_scores = model_scores.iloc[0].values  # scores of the best model\n",
    "model_2_scores = model_scores.iloc[1].values  # scores of the second-best model\n",
    "\n",
    "differences = model_1_scores - model_2_scores\n",
    "\n",
    "n = differences.shape[0]  # number of test sets\n",
    "dof = n - 1\n",
    "\n",
    "t_stat, p_val = compute_corrected_ttest(\n",
    "    differences,\n",
    "    dof,\n",
    "    num_samples_train,\n",
    "    num_samples_test,\n",
    ")\n",
    "print(f\"Corrected t-statistic: {t_stat:.3f}\")\n",
    "print(f\"Corrected p-value: {p_val:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test all models against the best model.\n",
    "best_model_scores = model_scores.iloc[0].values\n",
    "\n",
    "n_comparisons = model_scores.shape[0] - 1\n",
    "\n",
    "pairwise_t_test = []\n",
    "\n",
    "for model_i in range(1, len(model_scores)):\n",
    "    model_i_scores = model_scores.iloc[model_i].values\n",
    "    differences = model_i_scores - best_model_scores\n",
    "    t_stat, p_val = compute_corrected_ttest(\n",
    "        differences,\n",
    "        dof,\n",
    "        num_samples_train,\n",
    "        num_samples_test,\n",
    "    )\n",
    "\n",
    "    # Implement Bonferroni correction\n",
    "    p_val *= n_comparisons\n",
    "\n",
    "    # Bonferroni can output p-values higher than 1\n",
    "    p_val = 1 if p_val > 1 else p_val\n",
    "\n",
    "    pairwise_t_test.append([\n",
    "        model_scores.index[0],\n",
    "        model_scores.index[model_i],\n",
    "        t_stat,\n",
    "        p_val,\n",
    "    ])\n",
    "\n",
    "pairwise_comp_df = pd.DataFrame(\n",
    "    pairwise_t_test,\n",
    "    columns=[\"model_1\", \"model_2\", \"t_stat\", \"p_val\"],\n",
    ").round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_comp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_i in range(1, len(model_scores)):\n",
    "    model_i_scores = model_scores.iloc[model_i].values\n",
    "    differences = model_i_scores - best_model_scores\n",
    "\n",
    "    name_i = model_scores.index[model_i]\n",
    "    name_best = model_scores.index[0]\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(model_i_scores, bins=30, alpha=0.5, label=name_i)\n",
    "    plt.hist(best_model_scores, bins=30, alpha=0.5, label=name_best)\n",
    "    plt.title(f'{name_i} vs {name_best}')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(differences, bins=30, alpha=0.5)\n",
    "    plt.title(f'differences {name_i} - {name_best}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtragem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_filter = np.where(X[\"absences\"] % 2 == 0)\n",
    "y_filter = np.where(y != 0)\n",
    "df_filter = np.intersect1d(X_filter, y_filter)\n",
    "df_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X.iloc[df_filter],\n",
    "    y.iloc[df_filter],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_Ridge_100 = Pipeline([\n",
    "    (\"preprocessor\", preprocessing_pipeline),\n",
    "    (\"regressor\", Ridge(alpha=100)),\n",
    "])\n",
    "\n",
    "pipe_Ridge_100.fit(X_train, y_train)\n",
    "\n",
    "y_predict = pipe_Ridge_100.predict(X_test)\n",
    "rmse = np.sqrt(np.mean((y_test - y_predict)**2))\n",
    "\n",
    "f\"RMSE = {rmse}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coeficientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs_keys = pipe_Ridge_100[0].get_feature_names_out()\n",
    "coefs_values = pipe_Ridge_100.named_steps[\"regressor\"].coef_\n",
    "size = (len(coefs_keys), len(coefs_values))\n",
    "\n",
    "print(f\"Size: {size}\")\n",
    "\n",
    "coefs = pd.Series(\n",
    "    dict(\n",
    "        zip(coefs_keys, coefs_values)\n",
    "    )\n",
    ")\n",
    "\n",
    "coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
